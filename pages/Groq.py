import streamlit as st
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI  # ChatOpenAI suporta GROQ!
from dotenv import load_dotenv
import os

# Carregar vari√°veis de ambiente
load_dotenv()

# Configura√ß√£o da API GROQ
groq_api_key = os.getenv("GROQ_API_KEY")  # Defina isso no seu .env
llm = ChatOpenAI(
    model="llama-3.3-70b-versatile",  # Modelo da GROQ (outros: llama3-8b, gemma-7b)
    openai_api_key=groq_api_key,  # Usa a chave da API GROQ
    base_url="https://api.groq.com/openai/v1",  # URL da API GROQ
)

# Template de Prompt melhorado
prompt_template = """Voc√™ √© um assistente especializado em classificar transa√ß√µes financeiras em categorias predefinidas.
Categorias dispon√≠veis:
- Alimenta√ß√£o, Animais de Estima√ß√£o, Bares, C√¢mbio, Cashback, Compras, Constru√ß√£o, Contas, Cultura, Delivery Inter, Doa√ß√µes e Caridade, Drogaria, Educa√ß√£o, Ensino, Entretenimento, Esportes, Estacionamento, Gift Card, Hospedagem, Imposto/Juros/Multa, Inter, Inter Shop, Investimento, Lazer, Livrarias, Mercado, Moradia, Outras Sa√≠das, Outros, Pagamentos, Pet Shop, Presente, Recarga, Restaurantes, Sa√∫de, Seguros, Servi√ßos, Supermercado, Transporte, Vestu√°rio, Viagem, B√¥nus, Estorno, Outras Entradas, Renda, Rendimento, Vendas.

A transa√ß√£o recebida √©: "{input}"
Retorne apenas a categoria mais apropriada e uma breve justificativa.
Se for questionado qual modelo de LLM est√° sendo usado, tamb√©m responda.
"""

prompt = PromptTemplate.from_template(prompt_template)
model = prompt | llm  # Conecta o template ao LLM

# Interface do Streamlit
st.title("Classificador de Transa√ß√µes Financeiras üí∞")

# Hist√≥rico de mensagens
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

entrada = st.chat_input("Digite a descri√ß√£o da transa√ß√£o...")

if entrada:
    # Adiciona entrada do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": entrada})
    with st.chat_message("user"):
        st.write(entrada)

    # Obt√©m resposta do modelo
    resposta = model.invoke({'input': entrada}).content

    # Adiciona resposta do assistente ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": resposta})
    with st.chat_message("assistant"):
        st.write(resposta)
